# =============================================================================
# 1. DATA PREPARATION
# =============================================================================
import os
import re
import email
import numpy as np
import pandas as pd
from collections import Counter
import nltk
from bs4 import BeautifulSoup

# Custom transformer libraries for later use (scikit-learn)
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

# Load email file names from the local dataset directories
ham_fnames = sorted(os.listdir("./ham_zipped/main_ham"))
spam_fnames = sorted(os.listdir("./spam_zipped/main_spam"))

print("Number of ham files:", len(ham_fnames))
print("Number of spam files:", len(spam_fnames))
print("Spam to Ham ratio:", len(spam_fnames)/len(ham_fnames))
print("Spam percentage: ", (len(spam_fnames) / (len(spam_fnames) + len(ham_fnames))) * 100, "%")

# Function to parse an email file
def parse_email(fname, spam=False):
    directory = "./spam_zipped/main_spam" if spam else "./ham_zipped/main_ham"
    with open(os.path.join(directory, fname), "rb") as fp:
        return email.parser.BytesParser().parse(fp)

# Parse all emails into two lists
ham_emails = [parse_email(name) for name in ham_fnames]
spam_emails = [parse_email(name, spam=True) for name in spam_fnames]

# Function to extract email structure
def get_structure(email_obj) -> str:
    payload = email_obj.get_payload()
    if isinstance(payload, list):
        return "multipart({})".format(", ".join(get_structure(sub) for sub in payload))
    else:
        return email_obj.get_content_type()

# Function to count email structures in a list of emails
def email_structure_counter(emails):
    structs = Counter()
    for mail in emails:
        mail_struct = get_structure(mail)
        structs[mail_struct] += 1
    return structs

# Functions to convert HTML to text and to convert email to plain text
def html_to_text(email_obj) -> str:
    try:
        soup = BeautifulSoup(email_obj.get_payload(), "html.parser")
        plain = soup.text.replace("=\n", "")
        plain = re.sub(r"\s+", " ", plain)
        return plain.strip()
    except Exception:
        return "nothing"

def email_to_text(email_obj):
    text_content = ""
    for part in email_obj.walk():
        content_type = part.get_content_type()
        if content_type not in ['text/plain', 'text/html']:
            continue
        if content_type == 'text/plain':
            text_content += part.get_payload()
        else:
            text_content += html_to_text(part)
    return text_content

# Custom transformer to convert emails into word count dictionaries
class EmailToWordsCount(BaseEstimator, TransformerMixin):
    def __init__(self, strip_headers=True, to_lowercase=True, remove_punc=True, do_stem=True):
        self.strip_headers = strip_headers
        self.to_lowercase = to_lowercase
        self.remove_punc = remove_punc
        self.do_stem = do_stem
        self.stemmer = nltk.PorterStemmer()
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X, y=None):
        X_word_counts = []
        for email_obj in X:
            plain = email_to_text(email_obj)
            if plain is None:
                plain = "nothing"
            if self.to_lowercase:
                plain = plain.lower()
            if self.remove_punc:
                plain = plain.replace(".", "").replace(",", "").replace("!", "").replace("?", "").replace(";", "")
            word_counts = Counter(plain.split())
            if self.do_stem:
                stemmed_word_counts = Counter()
                for word, count in word_counts.items():
                    stemmed_word = self.stemmer.stem(word)
                    stemmed_word_counts[stemmed_word] += count
                word_counts = stemmed_word_counts
            X_word_counts.append(word_counts)
        return np.array(X_word_counts)

# Custom transformer to vectorize the word counts into fixed-length feature vectors
class WordCountVectorizer(BaseEstimator, TransformerMixin):
    def __init__(self, vocabulary_size=1000):
        self.vocabulary_size = vocabulary_size
    
    def fit(self, X, y=None):
        total_word_counts = Counter()
        for word_count in X:
            total_word_counts.update(word_count)
        self.most_common = total_word_counts.most_common(self.vocabulary_size)
        self.vocabulary_ = {word: i for i, (word, count) in enumerate(self.most_common)}
        return self
    
    def transform(self, X, y=None):
        X_new = np.zeros((X.shape[0], self.vocabulary_size + 1), dtype=int)
        # Additional column for words not in the vocabulary
        for row_idx, word_count in enumerate(X):
            for word, count in word_count.items():
                col_idx = self.vocabulary_.get(word, self.vocabulary_size)
                X_new[row_idx, col_idx] += count
        return X_new

